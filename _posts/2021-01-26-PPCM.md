---
layout: post
title: "Plug-and-Play Conversational Models"
description: "Controlling style and topics of large pre-trained conversational models"
comments: false
---
<style>

figcaption {
  /* background-color: black;
  color: white; */
  font-style: italic;
  padding: 2px;
  text-align: center;
}
.center {
  display: block;
  margin-left: auto;
  margin-right: auto;
  /* width: 70%; */
}
/* CSS Simple Pre Code */
pre {
    background: rgba(197, 225, 184, 0.2);
    /* white-space: pre; */
    /* word-wrap: break-word; */
    overflow: auto;
}

pre.code {
    /* margin: 1px 1px; */
    /* border-radius: 2px; */
    /* border: 1px solid #FDF1DD; */
    position: relative;
}

pre.code label {
    /* font-family: sans-serif; */
    /* font-weight: bold; */
    font-size: 13px;
    /* color: #ddd; */
    position: absolute;
    left: 12px;
    top: 9.5px;
    text-align: center;
    width: 20px;
    -webkit-user-select: none;
    -moz-user-select: none;
    -ms-user-select: none;
    pointer-events: none;
}

pre.code code {
    font-family: "Inconsolata","Monaco","Consolas","Andale Mono","Bitstream Vera Sans Mono","Courier New",Courier,monospace;
    display: block;
    margin: 0 0 0 25px;
    /* padding: 1px 16px 14px; */
    /* border-left: 1px solid #555; */
    overflow-x: auto;
    /* font-size: 13px; */
    /* line-height: 19px; */
    /* color: #ddd; */
}




</style>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

There has been considerable progress made towards conversational models that generate coherent and fluent responses, for example [DialoGPT](https://arxiv.org/abs/1911.00536), [Meena](https://arxiv.org/pdf/2001.09977.pdf) and [BlenderBot](https://arxiv.org/abs/2004.13637). These
large conversational models provide little control over the generated responses, and this control is further limited in the absence of annotated conversational datasets for attribute specific generation that can be used for fine-tuning the model. For example in Figure 1 the model, given the same dialogue history, could generate two very different responses, but it does not have any control mechanism to achieve this behaviour. In this short-blog, we highlight our recent work [Plug-and-Play Conversational Models](https://arxiv.org/pdf/2010.04344.pdf) on controlling large conversational models without using dialogue specific datasets and without fine-tuning a large model.

<br />
<br />
<img class="center"  width="99%" src="/assets/img/example_control.png" alt="...">
<figcaption>Figure 1. Example of controllable response generation. Give the same dialogue history, the model could generate the two different responses. The aim of this blog, is to show how to control the response style. </figcaption>
<br />
<br />

Controlling the response style and topic of open domain conversational models is important for two reasons: 1) **personalization**, for example, we could tune the model to fit the user's preference in term of response style, and 2) **engagingness**, we could force the model to be cheerful and interested to the user (e.g., by asking more questions). Existing large pre-trained dialgoue models have no direct control over Response style (Pos/Neg), Topics (Sport/Tech), Engagement (Asking question) and Toxic Responses. How can we control the response of a large pre-trained conversational model?

In our recent paper, [Plug-and-Play Conversational Models](https://arxiv.org/pdf/2010.04344.pdf) (PPCM), we first show the effectiveness of existing controllable generation methods such as plug-and-play language models ([PPLM](https://arxiv.org/abs/1912.02164)) in large pre-trained conversational models using a variety of styles and topics such as Positive, Negative, Question, Sport, Business/Finance, without using dialogue specific dataset. Then, to cope with the computational cost at the decoding time of [PPLM](https://arxiv.org/abs/1912.02164), we propose to distill the generated style/topic consistent responses by [PPLM](https://arxiv.org/abs/1912.02164) into a set of [residual adapters](https://arxiv.org/pdf/1902.00751.pdf) for directly learning how to steer the original distribution towards the selected attribute. We synthesize our methods in three steps: discriminator training, PPLM response generation, and Adapter training. Figure 2 summarizes this process. 



<img class="center"  width="75%" src="/assets/img/PPCM_steps.png" alt="...">
<figcaption>Figure 2. The three main steps of PPCM. </figcaption>
<br />


Since our experiments are highly based on PPLM, we strongly recommend to read the [PPLM Blog](https://eng.uber.com/pplm/) which provides a clear and compact explaination of the method. Moreover, in all our experiments and this blog, we use [DialoGPT](https://arxiv.org/abs/1911.00536) medium but this the proposed methodology is model agnostic, and can be applied to any language model style conversation systems. 

### Discriminator 
The discriminator is a linear classifier $$f$$ trained on an annotated dataset with sentence and label pairs as $$(x, y)$$ – note that these sentences do not necessarily need to be conversational responses, as in our case. For each sentence $$x$$ of length $$t$$, we compute the set of hidden states $$H$$ from DialoGPT.Then we compute the mean across time, and finally we train $$f$$ using the cross-entropy between the label distribution $$y$$ and $$f(H)$$. In our experiments, We train three discriminators covering six attribute models such as Positive, Negative, Question, Sci/Tech, Business and Sport. For controlling positive and negative responses, we use [SST-5](https://www.aclweb.org/anthology/D13-1170/) with the class Very-Positive and VeryNegative as the attribute. For controlling for Question, we use the speech-act annotation from [Daily Dialogue](https://www.aclweb.org/anthology/I17-1099/) with the Question class as the attribute. To avoid any dialogue related data, we only use the sentences without the corresponding context. Finally, for generating the response about Sci/Tech, Business and Sport, we use the [AG-NEWS](https://arxiv.org/abs/1509.01626) topic-classification dataset, using the respective classes as attributes. Table 2 in the main paper, shows the sample size statistics and the performance in terms of F1-score for all the aforementioned datasets. In our experiments, we noticed that a strong discriminator was not essential for good performance, thus we did not tune much the discriminator. 


### PPLM 
[PPLM](https://arxiv.org/abs/1912.02164) uses an attribute model (i.e., a classifier) for controlling the generated text of Language Model (i.e., GPT-2). Differently from [PPLM](https://arxiv.org/abs/1912.02164), where a set of predefined prefixes are used to trigger the generation, in our experiments we use 100 conversations ([Adiwardana et al., 2020](https://arxiv.org/pdf/2001.09977.pdf)) for generating 1100 possible prefixes (i.e., moving window of size two). These
open-domain generic dialogues serve as a prefix to trigger the responses rather than fine-tuning. Then, we run PPLM the different discriminators that we trained in previous step. Figure 3 shows a high-level intuition of this process. DialoGPT is used as the base language model $$P(x)$$ and by plugging different discriminators -- $$P(a=\textrm{POS}|x)$$ and $$P(a=\textrm{NEG}|x)$$ -- the model generate attribute coherent responses. 

<img class="center"  width="95%" src="/assets/img/PPLM.png" alt="...">
<figcaption>Figure 3. PPLM applied DialoGPT for controlling the response style. Dialogue prefixes are taken from <a link="https://arxiv.org/pdf/2001.09977.pdf">(Adiwardana et al., 2020)</a> and DialoGPT remains frozen.  </figcaption>
<br />



### Adapter 
 

<img class="center"  width="95%" src="/assets/img/ADPT.png" alt="...">
<figcaption>Figure 2. The three main steps of PPCM. </figcaption>
<br />




### Some Examples


### Conclusion
In this short blog, we demonstrate the potential of LM priming few-shot learning in the most common task-oriented dialogue system tasks (NLU, DST, ACT and NLG). Our experiments show that in most of the tasks larger LMs are better few-shot learners, confirming the hypothesis in [Brown TB et.al, ‎2020](https://arxiv.org/pdf/2005.14165.pdf) and, in some cases, they can also achieve similar or better results than the weakest finetuning-based baseline. Finally, we unveil two limitations of the current LM priming few-shot learning the computational cost and the limited word context size.

### Acknowledgements
I would like to thanks [Jason Wu](https://jasonwu0731.github.io/) for providing an easy to use code in ToD-BERT and for clarification about the code and tasks, [Baolin Peng](https://scholar.google.com/citations?user=u1CNjgwAAAAJ&hl=zh-CN) for the easy to use repository FewShotNLG and for providing help with the scorer, and [Sumanth Dathathri](https://dathath.github.io/) for the discussion and insight about the limitation of the LM priming few-shots. 
 
### Useful Links
- Github: [https://github.com/andreamad8/TASK-ORIENTED-LM-FEWSHOT](https://github.com/andreamad8/TASK-ORIENTED-LM-FEWSHOT)
- Paper: [https://arxiv.org/abs/2008.06239](https://arxiv.org/abs/2008.06239)
- Medium Blog: [https://medium.com/@madottoandrea/language-model-as-few-shot-learner-for-task-oriented-dialogue-systems-db4765796744](https://medium.com/@madottoandrea/language-model-as-few-shot-learner-for-task-oriented-dialogue-systems-db4765796744)
